{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from googleapiclient.discovery import build\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Query_result(object):\n",
    "    def __init__(self, title=None, link=None, snippet=None):\n",
    "        self.title = title\n",
    "        self.link = link\n",
    "        self.summary = snippet\n",
    "        self.is_relevant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query_terms):\n",
    "    '''\n",
    "    Search by calling Google Custom Search API\n",
    "    with input query terms. \n",
    "    Return a list of (10) search results.\n",
    "    '''\n",
    "    # Call Google Custom Search API\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=JSON_API_KEY)\n",
    "    query = ' '.join(query_terms)\n",
    "    res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID,).execute()\n",
    "    \n",
    "    # Parse returned query results\n",
    "    result = []\n",
    "    for item in res['items']:\n",
    "        # unicode to utf-8\n",
    "        title = item['title'].encode('utf-8').strip()\n",
    "        link = item['link'].encode('utf-8').strip()\n",
    "        snippet = item['snippet'].encode('utf-8').strip()\n",
    "        t = Query_result(title, link, snippet)\n",
    "        result.append(t)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feedback(query_result, query):\n",
    "    '''\n",
    "    Print query results out, \n",
    "    Add user's feedback information\n",
    "    to Query_result.label attribute\n",
    "    '''\n",
    "    print \"Parameters:\"\n",
    "    print \"Client Key = {}\\nEngine Key = {}\\nQuery = {}\\nPrecision = {}\".format(JSON_API_KEY, SEARCH_ENGINE_ID, ' '.join(query), PREC)\n",
    "    print \"Google Search Results:\"\n",
    "    print \"======================\"\n",
    "    \n",
    "    # Print each query result\n",
    "    for i, v in enumerate(query_result):\n",
    "        print \"\\nResult #{}:\".format(i+1)\n",
    "        print \"[\"\n",
    "        print \"URL: {}\\n\".format(v.link)\n",
    "        print \"Title: {}\\n\".format(v.title)\n",
    "        print \"Summary: {}\\n\".format(v.summary)\n",
    "        print \"]\\n\"\n",
    "        # Receive user's feedback\n",
    "        feedback = raw_input(\"Relevant ([Y]/N)?\")\n",
    "        assert(feedback == '' or feedback.lower() in 'yn')\n",
    "        if feedback == '' or feedback.lower() == 'y':\n",
    "            v.is_relevant = True\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_query(query_result, query, alpha=0.75, beta=0.15):\n",
    "    '''\n",
    "    According to the user's feedback,\n",
    "    derive new terms for query,\n",
    "    and adjust order of query terms.\n",
    "    '''\n",
    "    N = len(query_result)\n",
    "    re_vectors, irre_vectors, doc_freq = [], [], defaultdict(set)\n",
    "    \n",
    "    # Count term frequency and document frequency for each term in each document\n",
    "    print \"Indexing results ...\"\n",
    "    for i, v in enumerate(query_result):\n",
    "        vector = defaultdict(int)\n",
    "        terms = regularize(v.title + ' ' + v.summary) # terms: all terms in a document\n",
    "        for term in terms:\n",
    "            doc_freq[term].add(i)\n",
    "            vector[term] += 1\n",
    "        if v.is_relevant:\n",
    "            re_vectors.append(vector)  \n",
    "        else:\n",
    "            irre_vectors.append(vector)         \n",
    "    \n",
    "    # After this loop, every vector, each representing a document,\n",
    "    # will store the tf-idf value for each term in this document\n",
    "    for vector in re_vectors + irre_vectors:\n",
    "        for term in vector:\n",
    "            vector[term] = math.log(1+vector[term], 10) * math.log(float(N)/len(doc_freq[term]), 10) * 10000\n",
    "    \n",
    "    # Rocchio Algorithm -- combine all relevant and irrelevant vectors\n",
    "    print \"Indexing results ...\"\n",
    "    DR, DNR = len(re_vectors), len(irre_vectors)\n",
    "    new_vector = defaultdict(float)\n",
    "    for vector in re_vectors:\n",
    "        for term in vector:\n",
    "            new_vector[term] += vector[term] * alpha / DR \n",
    "    for vector in irre_vectors:\n",
    "        for term in vector:\n",
    "            new_vector[term] = max(0, new_vector[term] - vector[term] * beta / DNR)\n",
    "\n",
    "    # Print new_vector, for test purposes\n",
    "    pprint.pprint(sorted([(i, v) for i, v in new_vector.iteritems()], key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "    # Find (up to) 2 \"new\" terms in new_vector and add them to query terms\n",
    "    first, second, first_val, second_val = '', '', 0, 0\n",
    "    for term in new_vector:\n",
    "        if term not in query and new_vector[term] > 0: # pass terms that are already in query terms\n",
    "            weight = new_vector[term]\n",
    "            if weight > first_val:\n",
    "                first, first_val, second, second_val = term, weight, first, first_val\n",
    "            elif weight > second_val:\n",
    "                second, second_val = term, weight\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    if first: query.append(first)\n",
    "    if second: query.append(second)    \n",
    "    print \"Augmenting by {}\".format(first + ' ' + second)\n",
    "    \n",
    "    # Arrange new order of query termsï¼š\n",
    "    # \n",
    "    query = ORIGIN_QUERY.rstrip('\\n').split() + \\\n",
    "    map(lambda x: x[0], sorted([(t, new_vector[t]) for t in query if t not in ORIGIN_QUERY], \\\n",
    "                               key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularize(string):\n",
    "    '''\n",
    "    Discard all punctuations and stopwords, \n",
    "    convert all letters to lower case.\n",
    "    '''\n",
    "    return [word for word in re.sub(r'[^a-zA-Z0-9_ ]', '', string).lower().strip().split()\\\n",
    "            if word not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    '''\n",
    "    Get a set of stopwords.\n",
    "    '''\n",
    "    with open('./proj1-stop.txt') as f:\n",
    "        return set(word.rstrip('\\n') for word in f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_default_input():\n",
    "    '''\n",
    "    Generate default input only for test purposes.\n",
    "    '''\n",
    "    SEARCH_ENGINE_ID = \"018403154494399932789:5hcq8v_uic8\"\n",
    "    JSON_API_KEY = \"AIzaSyA9gBRthSU9gOiOEtlLfN2NEAI4lxZQxTE\"\n",
    "    PREC = '0.9'\n",
    "    ORIGIN_QUERY = 'per se'\n",
    "    ORIGIN_QUERY = 'brin'\n",
    "    ORIGIN_QUERY = 'columbia'\n",
    "    return JSON_API_KEY, SEARCH_ENGINE_ID, PREC, ORIGIN_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Main function.\n",
    "    '''\n",
    "    # Parse Arguments\n",
    "    global JSON_API_KEY, SEARCH_ENGINE_ID, PREC, ORIGIN_QUERY, STOPWORDS\n",
    "#     JSON_API_KEY, SEARCH_ENGINE_ID, PREC, origin_query = sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5]\n",
    "    JSON_API_KEY, SEARCH_ENGINE_ID, PREC, ORIGIN_QUERY = gen_default_input()\n",
    "    PREC = float(PREC)\n",
    "    ORIGIN_QUERY = ORIGIN_QUERY.lower()  # Record original query terms\n",
    "    STOPWORDS = get_stopwords()\n",
    "    query = ORIGIN_QUERY.rstrip('\\n').split()\n",
    "    \n",
    "    ### MAIN LOOP ###\n",
    "    while True:\n",
    "        query_result = search(query)\n",
    "        if len(query_result) < 10:\n",
    "            print \"Not enough query results. Stop.\"\n",
    "            return\n",
    "        get_feedback(query_result, query)\n",
    "        print \"======================\"\n",
    "        print \"FEEDBACK SUMMARY\"\n",
    "        print \"Query {}\".format(' '.join(query))\n",
    "        prec = sum(q.is_relevant for q in query_result) / float(len(query_result))\n",
    "        print \"Precision {}\".format(prec)\n",
    "        if prec >= PREC:  # Reached goal, exit\n",
    "            print \"Desired precision reached, Done!\"\n",
    "            return\n",
    "        elif prec == 0.0:  # Precision too low, exit\n",
    "            print \"Precision reached zero, stop.\"\n",
    "            return\n",
    "        print \"Still below the desired precision of {}\".format(PREC)\n",
    "        query = modify_query(query_result, query)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "Client Key = AIzaSyA9gBRthSU9gOiOEtlLfN2NEAI4lxZQxTE\n",
      "Engine Key = 018403154494399932789:5hcq8v_uic8\n",
      "Query = columbia\n",
      "Precision = 0.9\n",
      "Google Search Results:\n",
      "======================\n",
      "\n",
      "Result #1:\n",
      "[\n",
      "URL: http://www.columbia.com/\n",
      "\n",
      "Title: Columbia Sportswear: Outdoor Clothing, Outerwear & Accessories\n",
      "\n",
      "Summary: Shop direct from Columbia Sportswear. Our Outerwear is Tested Tough in the \n",
      "Pacific NW. Shop for Jackets, Pants, Shirts, Shoes & more.\n",
      "\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdr34234\\\\&^&*^(&*v\\\\df\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sdR34234\\&^&*^(&*v\\df\\n'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = defaultdict(float)\n",
    "print a['ss'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ss' in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
